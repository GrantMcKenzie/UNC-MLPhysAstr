{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML5YoNrZS_Gc"
   },
   "source": [
    "### This is a simple notebook to train a Support Vector Machine to discriminate between two types of collisional events. This week, we'll deal with data preprocessing, try out a simple Linear SVM, and diagnose its performance.\n",
    "\n",
    "It accompanies Chapter 4 of the book.\n",
    "\n",
    "Data for this exercise were kindly provided by [Sascha Caron](https://www.nikhef.nl/~scaron/).\n",
    "\n",
    "Copyright: Viviana Acquaviva (2023)\n",
    "\n",
    "Modifications by Julieta Gruszko (2025)\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scz89F7WS_Ge"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.svm import SVC, LinearSVC # New algorithm!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-V-0pnD9S_Gf"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "rc('text', usetex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTvCct6rS_Gg"
   },
   "source": [
    "### The first part of this notebook walks through the manipulation I did to get the data in the format we need and to select a random sample to keep computation times more manageable.\n",
    "\n",
    "This csv data set is a bit tricky to deal with: it has two different delimeters, ';' and ','. \n",
    "\n",
    "The semicolons separate the first 5 columns, which apply to the full event, and are also used to separate the lists of info about each of the products detected. So using the semicolons as a separator would give you:\n",
    "$$\\texttt{numID; processID; weight; MET; METphi; P1\\_info; P2\\_info; P3\\_info; ...}$$\n",
    "\n",
    "Commas are used to separate info about each of the products, so for each of the \"Pn_info\" columns separated by semicolons, there's a comma-separated list of the label and 4-momentum values:\n",
    "$$\\texttt{Pn\\_info = Pn\\_label, P\\_E, Pn\\_pt, Pn\\_eta, Pn\\_phi}$$\n",
    "\n",
    "First, we'll make an array for the column labels, allowing for up to 19 products per event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an array of column labels\n",
    "names = np.array(['numID', 'processID', 'weight', 'MET', 'METphi'])\n",
    "names = np.append(names, [['P'+str(i)+'_type', 'P'+str(i)+'_E', 'P'+str(i)+'_pt', 'P'+str(i)+'_eta', 'P'+str(i)+'_phi'] for i in range(19)])\n",
    "print(names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can use python's regular expressions parsing engine to accept a more complex delimeter expression, so we can still read in the csv data file in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMESRMOuS_Gg",
    "outputId": "6a0db168-8f9e-4162-feb1-32e76a7a61e7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/TrainingValidationData.csv', delimiter=';|,', engine = 'python', names=names)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUXOw8hmS_Gh",
    "outputId": "6f7a78bc-3b36-46a5-ca1e-ca26a2741105",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the $\\texttt{describe}$ function to look at the numerical columns. The ATLAS detector calorimeter accepts tracks with $ -4.9 < \\eta < 4.9$ and any value of azimuthal angle ($-\\pi < \\phi < \\pi$). You can see nearly the full range of allowed values for these columns present in the P0 track. \n",
    "\n",
    "If you scroll all the way over to the final columns, you can see that we have some empty ones. We'll drop those below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPw8KyOPS_Gn"
   },
   "outputs": [],
   "source": [
    "X = df.drop(['numID', 'processID', 'weight'], axis = 1) #drop indices, labels, and weights (used to weigh statistics in simulation studies of spectra)\n",
    "X = X.drop(['P18_type', 'P18_E', 'P18_pt', 'P18_eta', 'P18_phi'], axis = 1) #drop empty columns\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz7CfTQcS_Gn",
    "outputId": "ad529c19-dbea-48a9-de13-f9d59e60e1f0"
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQQbpVLmS_Go",
    "outputId": "f72074c1-5355-409a-cb5a-bc1e622ab6bd"
   },
   "outputs": [],
   "source": [
    "X.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9djrPJsFS_Gq"
   },
   "source": [
    "### Saving subset of the data to a new file\n",
    "\n",
    "We'll select 5000 random instances to reduce the size of the data set, reset the indices of both the features and labels, and store both to csv files. That will let us load the same data more easily if we want to work with it again (e.g. in next week's studio).\n",
    "\n",
    "\n",
    "First, we'll generate a random list of event indices to select 5000 events. We'll set the seed so everyone is using the same subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "sel = np.random.choice(df.shape[0], 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll select those rows from the feature data frame and save the subset to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OoIekILS_Gq"
   },
   "outputs": [],
   "source": [
    "features = X.iloc[sel,:]\n",
    "\n",
    "print(features.shape)\n",
    "print(features.columns)\n",
    "\n",
    "# reset index\n",
    "features.reset_index(drop=True, inplace=True)\n",
    "features.head()\n",
    "\n",
    "# Export the feature data to a file\n",
    "features.to_csv('../Data/ParticleID_features.csv', columns=features.columns, index_label= 'ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2FRSAe9S_Gs"
   },
   "source": [
    "#### Now, the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlRvXY0pS_Gs"
   },
   "outputs": [],
   "source": [
    "#Select the labels\n",
    "y = df.processID[sel].values # values makes it an array\n",
    "print(y)\n",
    "#Export labels to file\n",
    "np.savetxt('../Data/ParticleID_labels.txt', y, fmt = '%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGSpyK91S_Gu"
   },
   "source": [
    "## Now we're ready to start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G8H8VDBS_Gu"
   },
   "source": [
    "Read in features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xd7WnozpS_Gu"
   },
   "outputs": [],
   "source": [
    "features = pd.read_csv('../Data/ParticleID_features.csv', index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHs0SGxUS_Gu",
    "outputId": "32a6103a-31d2-413d-f3ab-9eb7a10c2c9f"
   },
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAl-0b3ES_Gv",
    "outputId": "3b896999-0a88-4101-e869-42ce0c14d3c8"
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82IFUA9YS_Gv"
   },
   "outputs": [],
   "source": [
    "y = np.genfromtxt('../Data/ParticleID_labels.txt', dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-Y9Mg5XS_Gv",
    "outputId": "0518e8db-5e1d-44c4-dbb5-1d500511eff7"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EQH55WhS_Gv"
   },
   "source": [
    "#### We need to turn categorical (string-type) labels into an array, e.g. 0/1.\n",
    "\n",
    "sk-learn has a nice preprocessing tool we can use for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzB-AaSrS_Gv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder() #turns categorical into 1 ... N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQSG8hh7S_Gw",
    "outputId": "14e4c4ed-585a-405e-b59a-ab66f9d3f670"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMpkms7qS_Gw"
   },
   "outputs": [],
   "source": [
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DwLiw0mS_Gw",
    "outputId": "8ceab38c-aa77-4769-eafb-7755815c574f"
   },
   "outputs": [],
   "source": [
    "y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our transformer used 1 for the first instance, but we actually wanted 4top to be the positive label, so we'll flip the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7WQyDGwS_Gx"
   },
   "outputs": [],
   "source": [
    "target = np.abs(y - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wsr1pOy8S_Gx",
    "outputId": "a01e48ff-95e6-432a-e7a5-11e6a964f356"
   },
   "outputs": [],
   "source": [
    "target # Happier now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTJcgL79S_Gx"
   },
   "source": [
    "#### Let's take a look at these features, using the \"describe\" property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asuEVhroS_Gy",
    "outputId": "178caa3d-71fc-4db0-c289-3156a303d355"
   },
   "outputs": [],
   "source": [
    "features.describe() #Note that this automatically excludes non-numerical type columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XLEV3HDS_Gy"
   },
   "source": [
    "### Important:\n",
    "\n",
    "Looking at the \"count\" row, we can see that the whole data set has 5,000 rows, but some columns are present only for a fraction of them. This is because of the variable number of products in each collision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoQXel9US_Gy"
   },
   "source": [
    "#### Option 1: Only consider the missing energy features and the 4-momentum values of the first four products, so we have limited imputing/manipulation problems.\n",
    "\n",
    "In our data set, the products are ordered by their energy, so choosing the first 4 should give us a lot of the relevant information.\n",
    "\n",
    "For the moment, we'll work just with the numerical features. Next week, we'll see some options for incorporating the track types as additional features.\n",
    "\n",
    "We have a trade-off between keeping more features, but having a more severe missing data/imputing problem, or keeping fewer features, but dealing with a simpler imputing problem. We are choosing the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_L-a_R5S_Gy"
   },
   "outputs": [],
   "source": [
    "features_lim = features[['MET', 'METphi', 'P0_E', 'P0_pt', 'P0_eta', 'P0_phi', 'P1_E', 'P1_pt', 'P1_eta', 'P1_phi', 'P2_E', 'P2_pt', 'P2_eta', 'P2_phi', 'P3_E', 'P3_pt', 'P3_eta', 'P3_phi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IV1hb_t2S_Gy",
    "outputId": "f3601d2c-6449-4fe3-fa7c-afc5d0849f36"
   },
   "outputs": [],
   "source": [
    "features_lim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-Lf0IDwS_Gz",
    "outputId": "47579686-b9af-4d2a-8354-4fe630c86c45",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_lim.describe() #This automatically excludes non-numerical type columns, and missing values/NaNs are not counted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkkYvd7PS_Gz"
   },
   "source": [
    "There are still some feature columns with different length! This means there might be NaN values. Let's replace them with 0 for the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZYyTYhrS_Gz"
   },
   "outputs": [],
   "source": [
    "features_lim = features_lim.fillna(0) #Fill with 0 everywhere there is a NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is the simplest but worst possible choice - imputing a constant value skews the model :D One step up would be to input the mean or median for each column. However, because only a limited number of instances have missing data, the choice of imputing strategy doesn't matter too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bGVvOk0S_Gz"
   },
   "source": [
    "#### Let's see what \"describe\" says now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbnrvY0rS_G0",
    "outputId": "87922e80-d595-4dc9-da1b-34927e2ef184"
   },
   "outputs": [],
   "source": [
    "features_lim.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6Svm-ZvS_G0"
   },
   "source": [
    "Yay - we now have consistent sizes, so we can use these as feature arrays, BUT be mindful of possible negative impacts of our imputing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8EHnF1OS_G0"
   },
   "source": [
    "### Let's move onto a quick exploration of labels and benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCtTNHcxS_G0",
    "outputId": "aafd29ac-f80d-40eb-8e50-8d4787aa1d3c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sum(target)/len(target) #distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvyMzF00S_G0"
   },
   "source": [
    "84\\% in the negative label, 16\\% in the positive label. \n",
    "\n",
    "This means that a classifier that puts everything in the negative class will have 84\\% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fIqGwitS_G0"
   },
   "source": [
    "How about a random classifier that just assigns a random value according to class distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b1s2iKUS_G0",
    "outputId": "84c52ae9-fa4b-4f68-a481-2dc7d8eed3b0"
   },
   "outputs": [],
   "source": [
    "#Numerical solution\n",
    "\n",
    "acc=0\n",
    "for i in range(1000):\n",
    "    x = np.random.choice(target,5000)\n",
    "    acc += metrics.accuracy_score(target,x)\n",
    "print(acc/1000)\n",
    "\n",
    "#Analytic solution \n",
    "\n",
    "print(0.8378*(0.8378) + 0.1622*0.1622)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "Describe what the \"numerical solution\" code is doing to get an estimated accuracy for a random classifier. You can give a description or pseudocode, your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, a \"random\" classifier would have 73% accuracy; a \"lazy\" classifier that predicts the most frequent class would have 83% accuracy. These are useful in order to set the expectation for what \"a good result\" is and what constitutes a significant improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve69kWKjS_G1"
   },
   "source": [
    "### Let's start with a linear model; model = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwpuwrSES_G1"
   },
   "source": [
    "Define a cross-validation strategy; establish benchmark for a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixh-JsVOS_G1"
   },
   "outputs": [],
   "source": [
    "bmodel = LinearSVC(dual = False) #Prefer dual=False when n_samples > n_features. If not, will not converge!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvi2LG4YS_G1"
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=101) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY7cAqlIS_G2"
   },
   "outputs": [],
   "source": [
    "l_benchmark_lim = cross_validate(bmodel, features_lim, target, cv = cv, scoring = 'accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xrWTbnlS_G2",
    "outputId": "228da934-1c99-4729-f3a3-637b5ee17077"
   },
   "outputs": [],
   "source": [
    "l_benchmark_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZpp047LS_G2",
    "outputId": "1f13b55d-6aeb-4bd9-f8f1-ad1a628d9542"
   },
   "outputs": [],
   "source": [
    "np.round(l_benchmark_lim['test_score'].mean(),3), np.round(l_benchmark_lim['test_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Evaluate the performance of the inital Linear SVM you trained. Does it out-perform the \"lazy\" or random classifiers described above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkG_9mL5S_G2"
   },
   "source": [
    "We can also check the predicted labels and the confusion matrix. Cross\\_val\\_predict will compile labels predicted when each object was in the test fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VscalT-S_G2"
   },
   "outputs": [],
   "source": [
    "ypred_bench_lim = cross_val_predict(bmodel, features_lim, target, cv = cv)\n",
    "metrics.confusion_matrix(target,ypred_bench_lim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "How many 4-top events are missed by this classifier? How many non-4-top events are misidentified as 4-top events?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: is there perhaps something that we should have done before building the SVM model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oBiGeXoS_G2"
   },
   "source": [
    "### How about scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation notes: Technically, standardizing/normalizing data using the entire learning set introduces leakage between train and test set (the test set \"knows\" about the mean and standard deviation of the entire data set). Usually this is not a dramatic effect, but the correct procedure is to derive the scaler within each CV fold (i.e. after separating in train and test), only on the train set, and apply the same transformation to the test set. The model then becomes a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXgzxXWfS_G2"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline #This allows one to build different steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SaXQ-JkS_G2"
   },
   "outputs": [],
   "source": [
    "piped_model = make_pipeline(StandardScaler(), LinearSVC(dual = False)) #make a pipeline with standard scaler and linear SVM\n",
    "\n",
    "piped_model.get_params() #return the parameters of the full pipeline. You should see some associeted with the the scaler, and some with the SVM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a pipeline, when we use cross-validation, then for each fold, it will run all the steps of the pipeline. In this case, that means that for each fold, it:\n",
    "- applies standard scaling to the train data\n",
    "- trains the linear SVM using this scaled train set, returning the train score\n",
    "- scales the test data with standard scaling\n",
    "- makes predictions for the test data, returning the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_lim_piped = cross_validate(piped_model, features_lim, target, cv = cv, scoring = 'accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X2EZ0t4S_G2",
    "outputId": "21e5de21-5560-4714-d5d2-3a0e8edcf1bf"
   },
   "outputs": [],
   "source": [
    "benchmark_lim_piped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxC_wvRES_G3",
    "outputId": "1adce87f-995d-4f0c-e476-855bb759b1dd"
   },
   "outputs": [],
   "source": [
    "#get the mean and standard deviation of the test score\n",
    "np.round(benchmark_lim_piped['test_score'].mean(),3), np.round(benchmark_lim_piped['test_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean and standard deviation of the train score\n",
    "np.round(benchmark_lim_piped['train_score'].mean(),3), np.round(benchmark_lim_piped['train_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaaIzsRpS_G4"
   },
   "source": [
    "This is a significant improvement (woo-ooh!), and the comparison between test and train scores tells us already something about the problem that we have. We can formalize this by looking at the learning curves, which tell us both about gap between train/test scores, AND whether we need more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDn2AoUOS_G4"
   },
   "source": [
    "### Learning curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORDR8CfuS_HK"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=5,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5), scoring = 'accuracy', scale = False):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"# of training examples\",fontsize = 14)\n",
    " \n",
    "    plt.ylabel(\"Accuracy score\",fontsize = 14)\n",
    "    \n",
    "    if (scale == True):\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "#    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"b\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\",\n",
    "             label=\"Training score from CV\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Test score from CV\")\n",
    "\n",
    "    plt.legend(loc=\"best\",fontsize = 12)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NklRkuu1S_G4",
    "outputId": "644cd239-fb08-446d-c460-a3578c0bca4e"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(piped_model, 'Generalized Learning Curves, linear SVC model, no reg', features_lim, target, train_sizes = np.array([0.05,0.1,0.2,0.5,1.0]), cv = KFold(n_splits=5, shuffle=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2cIdAkWS_G4"
   },
   "source": [
    "### Linear SVM Conclusion Questions\n",
    "\n",
    "1. How does the classifier performance compare to \"lazy\" and random classifiers?\n",
    "2. Briefly explain why scaling the data improved the SVM performance. You may find it helpful to think back to the different behavior we observed with respect to scaling effects in the Decision Tree and kNN algorithms. \n",
    "3. Does the model suffer from high variance, high bias, or both? How do you know?\n",
    "4. Would you expect that having more data would improve the performance of the classifier? How do you know?\n",
    "5. Given your diagnosis from questions 3 and 4, what are some things we should try to improve this classifier's performance? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you're finished, submit this studio to Gradescope. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PHYS448",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
