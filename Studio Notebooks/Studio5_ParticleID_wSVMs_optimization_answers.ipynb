{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML5YoNrZS_Gc"
   },
   "source": [
    "### This is a follow-up to Studio 4, using SVMs to classify 4-top ATLAS events. \n",
    "In this notebook, you'll use nested cross-validation to optimize hyperparameters for SVM models, comparing to the benchmark linear model that you trained in Studio 4. You'll also add newly-engineered features to the model and test the resulting performance. \n",
    "\n",
    "It accompanies Chapter 4 of the book.\n",
    "\n",
    "Data for this exercise were kindly provided by [Sascha Caron](https://www.nikhef.nl/~scaron/).\n",
    "\n",
    "Copyright: Viviana Acquaviva (2023)\n",
    "Modifications by Julieta Gruszko (2025)\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Names:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scz89F7WS_Ge"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.svm import SVC, LinearSVC # New algorithm!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate, cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-V-0pnD9S_Gf"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "rc('text', usetex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGSpyK91S_Gu"
   },
   "source": [
    "## We'll begin by opening the feature and label files you prepared last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G8H8VDBS_Gu"
   },
   "source": [
    "Read in features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xd7WnozpS_Gu"
   },
   "outputs": [],
   "source": [
    "features = pd.read_csv('../Data/ParticleID_features.csv', index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHs0SGxUS_Gu",
    "outputId": "32a6103a-31d2-413d-f3ab-9eb7a10c2c9f"
   },
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAl-0b3ES_Gv",
    "outputId": "3b896999-0a88-4101-e869-42ce0c14d3c8"
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82IFUA9YS_Gv"
   },
   "outputs": [],
   "source": [
    "y = np.genfromtxt('../Data/ParticleID_labels.txt', dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-Y9Mg5XS_Gv",
    "outputId": "0518e8db-5e1d-44c4-dbb5-1d500511eff7"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EQH55WhS_Gv"
   },
   "source": [
    "#### As we did last week, we'll turn categorical (string-type) labels into an array, e.g. 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzB-AaSrS_Gv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder() #turns categorical into 1 ... N\n",
    "y = le.fit_transform(y)\n",
    "target = np.abs(y - 1) #flip the labels, so 4-top is 1 and t/t-bar is 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQSG8hh7S_Gw",
    "outputId": "14e4c4ed-585a-405e-b59a-ab66f9d3f670"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asuEVhroS_Gy",
    "outputId": "178caa3d-71fc-4db0-c289-3156a303d355"
   },
   "outputs": [],
   "source": [
    "features.describe() #Note that this automatically excludes non-numerical type columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XLEV3HDS_Gy"
   },
   "source": [
    "### Imputing Missing Data\n",
    "As in Studio 4, we'll keep just the first 4 products and fill any reamining missing values with 0's. You'll test other imputation strategies on the homework. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_L-a_R5S_Gy"
   },
   "outputs": [],
   "source": [
    "features_lim = features[['MET', 'METphi', 'P0_E', 'P0_pt', 'P0_eta', 'P0_phi', 'P1_E', 'P1_pt', 'P1_eta', 'P1_phi', 'P2_E', 'P2_pt', 'P2_eta', 'P2_phi', 'P3_E', 'P3_pt', 'P3_eta', 'P3_phi']]\n",
    "features_lim = features_lim.fillna(0) #Fill with 0 everywhere there is a NaN\n",
    "features_lim.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve69kWKjS_G1"
   },
   "source": [
    "### Let's first reproduce our benchmark linear model (with scaling) from Studio 4 so we have it for comparison; model = LinearSVC()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixh-JsVOS_G1"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline #This allows one to build different steps together\n",
    "piped_model = make_pipeline(StandardScaler(), LinearSVC(dual=False)) #make a pipeline with standard scaler and linear SVM\n",
    "cv = StratifiedKFold(n_splits = 5, shuffle=True, random_state=101)# make a 5-fold stratified cross-validation, setting shuffle to \"True\" and random state to 101\n",
    "\n",
    "benchmark_lim_piped = cross_validate(piped_model, features_lim, target, cv = cv, scoring = 'accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X2EZ0t4S_G2",
    "outputId": "21e5de21-5560-4714-d5d2-3a0e8edcf1bf"
   },
   "outputs": [],
   "source": [
    "benchmark_lim_piped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxC_wvRES_G3",
    "outputId": "1adce87f-995d-4f0c-e476-855bb759b1dd"
   },
   "outputs": [],
   "source": [
    "np.round(benchmark_lim_piped['test_score'].mean(),3), np.round(benchmark_lim_piped['test_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(benchmark_lim_piped['train_score'].mean(),3), np.round(benchmark_lim_piped['train_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2SvyIshS_G5"
   },
   "source": [
    "### Parameter optimization \n",
    "\n",
    "When we optimize parameters with a grid search, we choose the parameters that give the best test scores. This is different from what would happen with new data - to do this fairly, at no point of the training procedure we are allowed to look at the test labels. Therefore, we would need to do <b> nested cross validation </b> to avoid leakage between the parameter optimization and the cross validation procedure and properly evaluate the generalization error.\n",
    "\n",
    "If you don't do nested cross-validation, you'll unintentionally bias your generalization error estimate, as you'll be choosing the model you use based on the same test data you're using to evaluate the performance on new data. \n",
    "\n",
    "Since we're doing a lot of new things here, we'll first run an optimization <b> without </b> nested cross-validation. This is the approach you'd use to pick a set of optimal hyperparameters, which you could use to make predictions on new data.\n",
    "\n",
    "Then, we'll run the same optimization <b> with </b> nested cross-validation, which is the approach used to get the correct generalization error. Then we'll compare the results from the two approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for both steps:\n",
    "\n",
    "First we'll set up the model and hyperparameters to scan over, which are common to both approaches. We'll also set up 1 cross-validation, which will eventually be the outer one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNFoiueXS_G5",
    "outputId": "01da5aaf-c655-400b-e57b-0ee14a4066c6"
   },
   "outputs": [],
   "source": [
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=101) #1 layer of cross-validation\n",
    "\n",
    "piped_model = make_pipeline(StandardScaler(), SVC()) #now using the general SVC so I can change the kernel\n",
    "\n",
    "piped_model.get_params() #this shows how we can access parameters both for the scaler and the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a dictionary of parameter values that we'll run the optimization over.\n",
    "\n",
    "You'll notice that we're not using a linear kernel. That's because the RBF (Gaussian) kernel with a very large $\\gamma$ is equivalent to a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'svc__kernel':['poly', 'rbf'], \\\n",
    "              'svc__gamma':[0.00001,'scale', 0.01, 0.1], 'svc__C':[0.1, 1.0, 10.0, 100.0], \\\n",
    "              'svc__degree': [2, 4, 8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few questions:\n",
    "- Briefly describe what each hyperparameter in the dictionary does. \n",
    "- How many SVC's will be trained if we do 5-fold cross-validation for each combination of hyperparameters?\n",
    "- How many of the SVC's trained correspond to models that are actually distinct? Hint: there are degeneracies! E.g. does the \"degree\" parameter change anything if you're using the rbf kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFPZcVTwS_G5"
   },
   "source": [
    "### Now we'll run the optimization with just 1 layer of cross-validation.\n",
    "Note that this might take a while (~1 min on my laptop); the early estimates output by this cell may be misleading because more complex models (in particular high gamma) take longer. To speed things up, we're running 4 jobs in parallel.\n",
    "\n",
    "Notice that the $\\texttt{GridSearchCV()}$ function constructs our $\\texttt{model}$ object, but doesn't actually train any models! The training happens when we call $\\texttt{fit}$, as usual.\n",
    "\n",
    "Once you run this cell, the $\\texttt{model}$ object will have attributes $\\texttt{best\\_score\\_}$, $\\texttt{best\\_params\\_}$ and $\\texttt{best\\_estimator\\_}$, which give us access to the optimal estimator (printed out), as well as $\\texttt{cv\\_results\\_}$ that can be used to visualize the performance of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizing SVC: THIS IS NOT YET NESTED CV\n",
    "\n",
    "model = GridSearchCV(piped_model, parameters, cv = outer_cv, \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaR-DhChS_G5",
    "outputId": "eb8c3d83-200d-489c-dbb8-92aabb77eb43"
   },
   "outputs": [],
   "source": [
    "model.fit(features_lim,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model performed the best? Give its relevant hyperparameters. What was its accuracy score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEfppjOGS_G6"
   },
   "source": [
    "#### We can visualize the models in a data frame, and rank them according to their test scores.\n",
    "\n",
    "I like to look at the mean and std of the test scores, the mean of the train scores (so I can evaluate if they differ and the significance of the result), and also fitting time (we may pick a faster model instead of the best model if the scores are comparable)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tY_72jBHS_G7",
    "outputId": "a1ba6846-051c-48b8-f917-97db27f9b393"
   },
   "outputs": [],
   "source": [
    "scores_lim = pd.DataFrame(model.cv_results_)\n",
    "\n",
    "scores_lim.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lim[['params','mean_test_score','std_test_score','mean_train_score', \\\n",
    "            'mean_fit_time']].sort_values(by = 'mean_test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also isolate one type of kernel to look at it more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lim[scores_lim['param_svc__kernel'] == 'poly'][['params','mean_test_score','std_test_score',\\\n",
    "                        'mean_train_score','mean_fit_time']].sort_values(by = 'mean_test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few questions:\n",
    "- Why do some of the models have identical scores (e.g., the top 3 models)?\n",
    "- What hyperparameter values are common to all the best-performing models? \n",
    "- What hyperparameters do not strongly affect the accuracy? Ignore degenerate models in this discussion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we'll run nested cross-validation to get the generalization error.\n",
    "\n",
    "First, we need to make one more layer of cross-validation. We'll do 4 splits for the inner layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the cross-validation applied to the hyperparameter optimization step is the $\\texttt{inner\\_cv}$.\n",
    "\n",
    "Now, we'll use $\\texttt{cross\\_val\\_score}$ with $\\texttt{outer\\_cv}$ (instead of $\\texttt{fit}$!) to fit the model and get its score. \n",
    "\n",
    "This may be a bit confusing: remember that the $\\texttt{GridSearchCV()}$ is just a constructor, it doesn't run any model fits! \n",
    "\n",
    "\n",
    "I found this explanation, from Arpit Omprakash on StackOverflow (https://stackoverflow.com/a/78544053) extremeley helpful:\n",
    "\n",
    "\"In the first line of code here, we are instantiating the GridSearchCV object using the inner_cv cross validator (but not fitting it). \n",
    "\n",
    "In the second line, we are doing a lot of things. First, using cross_val_score and outer_cv we break the initial data into different splits, let's call it x_tr_0, x_ts_0, x_tr_1, x_ts_1, x_tr_2, x_ts_2, x_tr_3, x_ts_3, x_tr_4, x_ts_4, (since there are five splits, and in each split, we have training and testing data). The training data is passed on to the GridSearchCV method in each fold. So, the inner_cv cross validator works on the training data splits from the outer_cv cross validator. So, in the GridSearchCV method, we are basically breaking down x_tr_0 into 4 splits: x_tr_0_0, x_tr_0_1, x_tr_0_2, x_tr_0_3. In these \"inner\" splits we are doing the hyperparameter tuning.\n",
    "\n",
    "Once the optimal hyperparameters are calculated, we use these in the \"outer\" split for calculating the model performance. In this case, the model evaluation is done on the outer_cv split test data (which is unseen by the hyperparameter tuning \"inner\" split). This ensures that the performance values we are getting are more generalizable and there is no data leakage.\"\n",
    "\n",
    "Because I'm using $\\texttt{cross\\_val\\_score}$, all I'm returning is the scores of each of the 5 outer folds. If you want to return the models as well, you can do so with the $\\texttt{cross\\_validate}$ function.\n",
    "\n",
    "\n",
    "Because we're running 5 times as many fits as before, this will take a bit longer! It ran in 1.5 minutes on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_model = GridSearchCV(piped_model, parameters, cv = inner_cv, verbose = 2, n_jobs = 4, return_train_score=True)\n",
    "nested_score = cross_val_score(nested_model, X=features_lim, y=target, cv= outer_cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_score #an array of all the test scores for the outer CV, using the optimal hyperparameters from the inner CV step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(nested_score.mean(), 3), np.round(nested_score.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average accuracy and generalization error of the model? Is the accuracy you found for the optimal model (without using nested cross-validation) compatible with this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for a model with optimized hyperparameters are reported as: optimal model accuracy $\\pm$ generalization error\n",
    "\n",
    "Report your results for the optimized SVC model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiHw_EU-S_G7"
   },
   "source": [
    "### Diagnosis \n",
    "\n",
    "- Compare the performance of the best-performing model found in the optimization to the benchmark scaled Linear SVC model. Does it perform measurably better?\n",
    "\n",
    "- Our diagnosis of the scaled Linear SVC model was that it had high bias. Has this problem been corrected by making the model more complex using different hyperparameters?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8lVS2-mS_HC"
   },
   "source": [
    "The problem here is high bias, which is not that surprising given that we are using only a subset of features.\n",
    "\n",
    "We can try two things: making up new features which might help, based on what we know about the problem, and using an imputing strategy to include information about the discarded features. Here we'll focus on adding new engineered features, and you'll try the imputing approach on Homework 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prpkV4jgS_HC"
   },
   "source": [
    "### Next step: Feature Engineering\n",
    "\n",
    "First, we'll define some new variables.\n",
    "We'll go back to the full list of features (not the abbreviated list we tested above) to develop our new engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_A5nbG0S_HC"
   },
   "outputs": [],
   "source": [
    "features = features.fillna(0) #takes care of nan\n",
    "features = features.replace('', 0) #takes care of empty string values\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaAeaOZ8S_HD"
   },
   "source": [
    "#### Let's start by looking at what kind of particles we have as a product of the collision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXFQVWRuS_HD",
    "outputId": "7172b92a-c670-4515-f675-c4bddd2cac34"
   },
   "outputs": [],
   "source": [
    "# make a 2D numpy array of all the values of the particle type columns, storing the values as strings\n",
    "ptypes = np.array([features['P'+str(i)+'_type'].values for i in range(0,18)])\n",
    "\n",
    "print(ptypes)\n",
    "\n",
    "print(np.shape(ptypes)) #note that the shape might be the transpose of what you expect! There are 18 rows (one for each particle), 5000 columns (one for each instance)\n",
    "\n",
    "print(ptypes[0, 0:5]) #e.g. the type of particle 0 in instances 0 - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the unique values of the particle type columns\n",
    "np.unique(ptypes.astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIAKGgOqS_HE"
   },
   "source": [
    "#### Here are the proposed new features (justification can be found in Chapter 4).\n",
    "    \n",
    "    1. The total number of particles produced\n",
    "    2. The total number of b jets\n",
    "    3. The total number of jets\n",
    "    4. The total number of leptons (electrons, positron, mu+, mu-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of non-zero types for each instance\n",
    "\n",
    "ntot = np.array([(np.sum(np.array([ptypes[i][j] != 0 for i in range(ptypes.shape[0])]))) for j in range(features.shape[0])])\n",
    "ntot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGtLtW1-S_HE"
   },
   "outputs": [],
   "source": [
    "#define new column in my data frame\n",
    "features['Total_products'] = ntot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5TG2m0fS_HE"
   },
   "outputs": [],
   "source": [
    "#count number of b jets \n",
    "nbtot = np.array([np.sum(np.array([ptypes[i][j] == 'b' for i in range(ptypes.shape[0])])) for j in range(features.shape[0])])\n",
    "#define new column in my data frame\n",
    "features['Total_b'] = nbtot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3K5KSm4S_HE"
   },
   "outputs": [],
   "source": [
    "#You get the idea, let's count all types (jets, photons g, e-, e+, mu-, mu+)\n",
    "njtot = np.array([np.sum(np.array([ptypes[i][j] == 'j' for i in range(ptypes.shape[0])])) for j in range(features.shape[0])])\n",
    "ngtot = np.array([np.sum(np.array([ptypes[i][j] == 'g' for i in range(ptypes.shape[0])])) for j in range(features.shape[0])])\n",
    "\n",
    "# count each of the lepton types separately, then sum to get total leptons\n",
    "n_el_tot = np.array([np.sum(np.array([ptypes[i][j] == 'e-' for i in range(ptypes.shape[0])])) for j in range(features.shape[0])])\n",
    "n_pos_tot = np.array([np.sum(np.array([ptypes[i][j] == 'e+' for i in range(ptypes.shape[0])])) for j in range(features.shape[0])])\n",
    "n_muneg_tot = np.array([np.sum(np.array([ptypes[i][j] == 'm-' for i in range(ptypes.shape[0])])) for j in range(features.shape[0])])\n",
    "n_mupos_tot = np.array([np.sum(np.array([ptypes[i][j] == 'm+' for i in range(ptypes.shape[0])])) for j in range(features.shape[0])])\n",
    "n_lepton_tot = n_el_tot + n_pos_tot + n_muneg_tot + n_mupos_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAdBDl-US_HF"
   },
   "source": [
    "And here we define the other new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woyzSbSsS_HF"
   },
   "outputs": [],
   "source": [
    "features['Total_j'] = njtot\n",
    "features['Total_g'] = ngtot\n",
    "features['Total_leptons'] = n_lepton_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIC5oBplS_HF",
    "outputId": "83cdeb20-5f94-4590-882c-a755bc4e110d"
   },
   "outputs": [],
   "source": [
    "features.head() #scroll to the final columns to see your new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QaMinaQS_HF"
   },
   "source": [
    "### Feature engineering 1: impact of engineered variables\n",
    "\n",
    "We'll add these 4 new features to our original set to see if that improves our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y55xL6SQS_HF"
   },
   "outputs": [],
   "source": [
    "features_lim_2 = features[['MET', 'METphi', 'P0_E', 'P0_pt', 'P0_eta', 'P0_phi', \n",
    "                           'P1_E', 'P1_pt', 'P1_eta', 'P1_phi', \n",
    "                           'P2_E', 'P2_pt', 'P2_eta', 'P2_phi', \n",
    "                           'P3_E', 'P3_pt', 'P3_eta', 'P3_phi',\n",
    "                           'Total_products', 'Total_b' ,'Total_j','Total_g','Total_leptons']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll try our benchmark model (Standard Scaler and Linear SVC), using 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wHVYyKQS_HF",
    "outputId": "c1d03e50-e195-40ca-a59b-1dd74086d0d2"
   },
   "outputs": [],
   "source": [
    "piped_model #remember our benchmark model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEly33GaS_HG"
   },
   "outputs": [],
   "source": [
    "benchmark_lim2 = cross_validate(piped_model, features_lim_2, target, cv = cv, scoring = 'accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJGrC6XXS_HG",
    "outputId": "c8773996-1a76-48a4-9120-6f7d60996bb3"
   },
   "outputs": [],
   "source": [
    "benchmark_lim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZTxFHyWS_HG",
    "outputId": "403fca03-412e-4e80-cb88-fd42768ce02d"
   },
   "outputs": [],
   "source": [
    "np.round(benchmark_lim2['test_score'].mean(),3), np.round(benchmark_lim2['test_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(benchmark_lim2['train_score'].mean(),3), np.round(benchmark_lim2['train_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average accuracy and generalization error of the model? Note: we haven't done any parameter optimization here, so we don't need nested cross-validation to get the error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of this enhanced-feature Linear SVC to your optimized model. Which change had the larger impact on the bias: hyperparameter optimization or feature engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a very significant improvement, which cuts our error rate in half!\n",
    "\n",
    "In my experience, this knowledge-informed feature engineering is often very successful, more than hyperparameter optimization. Machine learning methods are often tooted for their ability to learn relevant representations, but non-deep-learning methods are less capable to do so, and providing informative features is very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4q395z_S_HG"
   },
   "source": [
    "We can optimize this model as well, just as we did before. For the moment we'll skip this for the sake of time, but you'll get lots of practice with this on HW 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1VSOK0QS_HG"
   },
   "source": [
    "### Techniques for Feature Engineering: One-Hot Encoding\n",
    "\n",
    "Another feature engineering attempt we could potentially do is use the type of product in the i-th location as a feature. To do this, we need to somehow turn the particle types into numerical features, since that's all SVM's know how to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUP4q0o0S_HG"
   },
   "source": [
    "We could do it with label encoding, as we did earlier in this notebook, but such strategy introduces a notion of distance metric (labels that are mapped to 0 and 1 are interpreted to be closer to each other than labels that are mapped into 0 and 7). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcGC971kS_HG"
   },
   "source": [
    "As an alternative, we can introduce as many new columns as possible values for each categorical variable we are re-mapping, and we just use a 0/1 to indicate that the particle is of that type. This is known as \"one-hot encoding,\" since only one of the categorical variable columns we add can ever be \"hot\" at a time (that is to say, a 1, instead of a 0). \n",
    "\n",
    "This is achieved with the wonderfully-named \"get_dummies\" function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXVez-RVS_HH"
   },
   "outputs": [],
   "source": [
    "features_add = pd.get_dummies(data=features, columns=['P'+str(i)+'_type' for i in range(0,18)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmJWxmvSS_HH",
    "outputId": "297b6c72-97f7-4789-caf1-d87662246dd7"
   },
   "outputs": [],
   "source": [
    "features_add.columns[77:90] #A subset of the new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice a couple of things:\n",
    "- Particle 0 only comes in type \"b\" or \"j\" -- this probably has to do with how the ATLAS collaboration decides to trigger on events. Presumably they're only storing events that have at least 1 jet in them (whether or not it's a b-jet). \n",
    "-  Empty particle tracks are now listed as \"type 0\" in the encoding scheme. This may be acceptable, or it could be something you decide to clean up on a later test, if you notice problems classifying events with missing tracks. Developing an ML method is often an iterative process! You don't need to have everything perfectly correct the first time through; try it and see what happens before investing a lot of time into perfecting things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIKHO39vS_HH",
    "outputId": "da6cff62-ce52-4651-d8d5-6f1436d62f0f"
   },
   "outputs": [],
   "source": [
    "features_add.shape\n",
    "\n",
    "features_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8ttkdAIS_HH"
   },
   "source": [
    "### Feature engineering 2: add other variables (type of product) for the first four particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3bi1jD-S_HH"
   },
   "outputs": [],
   "source": [
    "features_lim_3 = features_add[['MET', 'METphi', 'P0_E', 'P0_pt', 'P0_eta', 'P0_phi', \n",
    "                           'P1_E', 'P1_pt', 'P1_eta', 'P1_phi', \n",
    "                           'P2_E', 'P2_pt', 'P2_eta', 'P2_phi', \n",
    "                           'P3_E', 'P3_pt', 'P3_eta', 'P3_phi',\n",
    "                           'Total_products', 'Total_b' ,'Total_j','Total_g','Total_leptons',\n",
    "                           'P0_type_b', 'P0_type_j',\n",
    "                            'P1_type_0', 'P1_type_b', 'P1_type_e+', 'P1_type_e-', 'P1_type_g', 'P1_type_j', 'P1_type_m+', 'P1_type_m-', \n",
    "                            'P2_type_0', 'P2_type_b', 'P2_type_e+', 'P2_type_e-', 'P2_type_g', 'P2_type_j', 'P2_type_m+', 'P2_type_m-', \n",
    "                            'P3_type_0', 'P3_type_b', 'P3_type_e+', 'P3_type_e-', 'P3_type_g', 'P3_type_j', 'P3_type_m+', 'P3_type_m-']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klZrsWyWS_HH",
    "outputId": "b98b1c8a-6d14-4b7e-f07d-1e31030a94ad"
   },
   "outputs": [],
   "source": [
    "features_lim_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the benchmark model one more time, using the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUSpOnH8S_HH"
   },
   "outputs": [],
   "source": [
    "benchmark_lim3 = cross_validate(piped_model, features_lim_3, target, cv = cv, scoring = 'accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yErr6p0BS_HH",
    "outputId": "1645fffe-36a1-426a-828f-921045dfa7ee"
   },
   "outputs": [],
   "source": [
    "benchmark_lim3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvkMGPTWS_HH",
    "outputId": "ef20a2c0-d7f5-48be-c599-df5bc3efb585"
   },
   "outputs": [],
   "source": [
    "np.round(benchmark_lim3['test_score'].mean(),3), np.round(benchmark_lim3['test_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Nt9bTVSS_HI",
    "outputId": "cf136f1d-5ee8-460f-e1c4-b177abd90a7a"
   },
   "outputs": [],
   "source": [
    "np.round(benchmark_lim3['train_score'].mean(),3), np.round(benchmark_lim3['train_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average accuracy and generalization error of the model? Note: we haven't done any parameter optimization here, so we don't need nested cross-validation to get the error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of this enhanced-feature Linear SVC to the first feature-engineered model (the one with the numbers of products as added features). Do you see any improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QteCCaES_HI"
   },
   "source": [
    "#### Next, we would normally optimize the model. Again, we'll skip this for the sake of time in this Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9-ksftvS_HI"
   },
   "source": [
    "### Finally, we can try with all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8DOOEisS_HI",
    "outputId": "5f752255-8c8d-40fa-d1a7-af27cdf06740"
   },
   "outputs": [],
   "source": [
    "features_add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MF6IvpXS_HJ"
   },
   "outputs": [],
   "source": [
    "benchmark_all = cross_validate(piped_model, features_add, target, cv = cv, scoring = 'accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90Flpht2S_HJ",
    "outputId": "b477d9eb-6854-49d8-c884-5391ca91648d"
   },
   "outputs": [],
   "source": [
    "benchmark_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwv6ogF_S_HJ",
    "outputId": "6aecd261-2a4b-4af0-cfaa-d95db1f74f9a"
   },
   "outputs": [],
   "source": [
    "np.round(benchmark_all['test_score'].mean(),3), np.round(benchmark_all['test_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6Ieu1J1S_HJ",
    "outputId": "cda11944-f6bb-4f70-8c04-fde860a56416"
   },
   "outputs": [],
   "source": [
    "np.round(benchmark_all['train_score'].mean(),3), np.round(benchmark_all['train_score'].std(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average accuracy and generalization error of the model? Note: we haven't done any parameter optimization here, so we don't need nested cross-validation to get the error!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the bias and variance of this model (using all the features) to the other two models using engineered features. What happened to the variance as the number of features increased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2_MswU_S_HJ"
   },
   "source": [
    "We could run the optimization, but as you might have anticipated, it won't help much, and it is very time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKKD5s6PS_HK"
   },
   "source": [
    "### Take-home message: feature engineering often works best if we use subject matter knowledge, and buulding more features is not necessarily better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You're done! Upload your work to Gradescope."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PHYS448",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
